{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e84eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT는 현재 시점에서 텍스트 마이닝 딥러닝 모형 중 가장 중요한 모형이다. 텍스트 마이닝의 거의 전 분야에서 우수한 성능을 보이고 있다.\n",
    "# 언어 모델은 문장 혹은 단어의 시퀀스에 대해 확률을 할당하는 모델을 말한다.\n",
    "# BERT는 언어 모델 기반의 학습을 이용해 언어의 대한 이해를 높이는 데 목적이 있다. 물론 그 이해를 바탕으로 다양한 작업을 수행하는 것이\n",
    "# 최종 목적이다.\n",
    "# GPT가 단방향 혹은 순방향으로의 셀프 어텐션만 사용하는 것에 대해 BERT는 양방향 셀프 어텐션을 모두 활용할 수 있다.\n",
    "# GPT는 BERT보다 먼저 발표된 모형으로, 주로 자연어 문장 생성에 특화된 모델이다. 언어 모델로 학습이 되었고 이를 바탕으로 해서 생성 외에\n",
    "# 문서 분류와 같은 다른 분야에서도 좋은 성능을 냈는데, 트랜스포머의 디코더 부분만 따로 떼어서 학습 모형으로 사용했다.\n",
    "# 따라서 인코더에서 디코더로의 어텐션은 생략돼 있고, 셀프 어텐션은 순방향만 적용된다.\n",
    "# 반면, BERT는 트랜스포머의 인코더 부분을 떼어서 사용했다.\n",
    "\n",
    "# BERT의 학습은 사전학습과 미세조정학습의 두 단계로 나뉘어진다.\n",
    "# 사전학습은 언어에 대한 이해를 높이기 위한 비지도학습이고, 미세조정학습은 실제 수행하고자 하는 작업에 대한 지도학습이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8326581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감성분석 결과: POSITIVE, 감성스코어: 0.9999\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\")\n",
    "result = clf(\"what a beautiful day!\")[0]\n",
    "print('감성분석 결과: %s, 감성스코어: %0.4f' % (result['label'], result['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03d5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0394f55c9b9f4004afc2612e833aa6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leedonghoon\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Leedonghoon\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\Leedonghoon\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and felt that something would have to go off if she turned the whole island around and asked the next man if he had any clothes on them.\n",
      "\n",
      "\"Why didn\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 생성, 비교적 매끄럽게 문장이 만들어지는 것을 볼 수 있다.\n",
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "result = text_generator(\"Alice was beginning to get very tired of sitting by her sister on the bank,\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b300ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 29%\n",
      "yes: 71%\n"
     ]
    }
   ],
   "source": [
    "# 1이 True, 즉 '의미적으로 유사하다' 유사할 확률 71%\n",
    "\n",
    "# 자동 클래스를 이용한 토크나이저와 모형의 사용\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased-finetuned-mrpc\"\n",
    ")\n",
    "\n",
    "# 의미적으로 유사한 두 문장을 선언\n",
    "input_sentence = \"She angered me with her inappropriate comments, rumor-spreading, and disrespectfulness at the formal dinner table\"\n",
    "target_sequence = \"She made me angry when she was rude at dinner\"\n",
    "# 토큰화\n",
    "tokens = tokenizer(input_sentence, target_sequence, return_tensors=\"pt\")\n",
    "\n",
    "# 모형으로 결과를 예측\n",
    "logits = model(**tokens).logits\n",
    "\n",
    "# 소프트맥스를 이용해 결과값을 클래스에 대한 확률로 변환\n",
    "results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f'{label}: {int(round(results[i] * 100))}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cf2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no:95%\n",
      "yes:5%\n"
     ]
    }
   ],
   "source": [
    "# 전혀 관련 없는 문장 실행\n",
    "target_sequence = \"The boy quickly ran across the finish line, seizing yet another victory\"\n",
    "tokens = tokenizer(input_sentence, target_sequence, return_tensors=\"pt\")\n",
    "logits = model(**tokens).logits\n",
    "results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f'{label}:{int(round(results[i] * 100 ))}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7164a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Leedonghoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count: 1600\n",
      "Test set count: 400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# movie review data에서 file id를 가져옴\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "# file id를 이용해 raw text file을 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "# label을 0,1의 값으로 변환\n",
    "label_dict = {'pos':1, 'neg':0}\n",
    "y = np.array([label_dict[c] for c in categories])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews,y,test_size=0.2, random_state=7)\n",
    "print('Train set count:', len(X_train))\n",
    "print('Test set count:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a47139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 영화리뷰 감성분석 정확도: 0.8425\n"
     ]
    }
   ],
   "source": [
    "# BERT는 많은 메모리를 사용하므로 400개의 테스트 데이터셋을 한꺼번에 처리하는 것은 메모리가 아주 많은 경우가 아니라면 거의 불가능에 가깝다.\n",
    "# 따라서 10개씩 잘라서 모형을 돌리고 결과를 합쳐서 성능 살펴보기\n",
    "# 미세조정학습이 된 'distilbert-base-uncased-finetuned-sst-2-english' 모형을 가져와 사용함.\n",
    "# GPU 가속을 확용하기 위해 모형과 연산할 텐서를 GPU로 옮기고, 연산이 끝난 결과를 CPU로 가져오는 과정을 포함함.\n",
    "# 정확도는 84.25%로 나이브 베이즈(8장)로 얻은 79.7%보다 높은 값일 뿐만 아니라 CNN을 이용해 얻은 81.2%보다도 좋은 값이다.\n",
    "# 즉 사전학습된 모형을 그대로 사용하는 것이 아니라 내가 가진 학습 데이터를 사용해서 성능을 더 높이려는 노력이\n",
    "# 미세조정학습이다.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# cuda를 이용한 GPU 연산이 가능하면 cuda를 사용하고, 아니면 cpu를 사용\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'distilbert-base-uncased-finetuned-sst-2-english'\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased-finetuned-sst-2-english'\n",
    ")\n",
    "# 모델을 gpu로 옮겨서 연산을 준비\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 10 # 모형으로 한번에 예측할 데이터의 수\n",
    "y_pred = [] # 전체 예측결과를 저장\n",
    "\n",
    "num_batch = len(y_test) //batch_size\n",
    "\n",
    "for i in range(num_batch):\n",
    "    inputs = tokenizer(\n",
    "        X_test[i*batch_size:(i+1)*batch_size],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # 토큰화 결과를 GPU로 이동\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # 모형으로 결과를 예측\n",
    "    logits = model(**inputs).logits\n",
    "    \n",
    "    # 결괏값을 클래스에 대한 확률로 변환\n",
    "    pred = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # 예측결과를 CPU로 가져와서 넘파이로 변환한 후,\n",
    "    # argmax로 확률이 가장 큰 클래스를 선택함\n",
    "    results = pred.cpu().detach().numpy().argmax(axis=1)\n",
    "    \n",
    "    # 전체 예측결과에 추가\n",
    "    y_pred.extend(results.tolist())\n",
    "    \n",
    "#  gpu 메모리를 비움\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "score = sum(y_test == np.array(y_pred))/len(y_test)\n",
    "print(\"NLTK 영화리뷰 감성분석 정확도:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769fabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2082a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
