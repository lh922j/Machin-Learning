{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a816eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT í•™ìŠµì„ ìœ„í•œ ì „ì²˜ë¦¬\n",
    "# BERTì˜ ë‘ ê°€ì§€ ì–¸ì–´ ëª¨ë¸ í•™ìŠµì— ëŒ€ì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ì„¸ ê°œì˜ ì„ë² ë”©ì„ ì´ìš©í•œë‹¤.\n",
    "# ê·¸ ì¤‘ ì²«ì§¸ëŠ” í† í° ì„ë² ë”©ì´ë‹¤. ë¬¸ì¥ì— ì‚¬ìš©ëœ ë‹¨ì–´ ì™¸ì— ë‘ ì¢…ë¥˜ì˜ íŠ¹ìˆ˜ í† í°ì´ ì¶”ê°€ë¼ ìˆë‹¤. \n",
    "# ì²«ì§¸ëŠ” [CLS]í† í°: ë¶„ë¥˜ í† í°ìœ¼ë¡œ, í•œ ë¬¸ì„œì— ëŒ€í•œ ë¬¸ì„œ ë¶„ë¥˜ë‚˜ ë‘ ë¬¸ì„œì˜ ê´€ê³„ì— ëŒ€í•œ ë¶„ë¥˜ë¥¼ í•˜ê¸° ìœ„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ ìµœì¢…ì ìœ¼ë¡œ ì¶œë ¥ \n",
    "# ë‘˜ì§¸ëŠ” [SEP]í† í°: seperator í† í°ìœ¼ë¡œ í•œ ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ê±°ë‚˜ ë‘ ë¬¸ì¥ì„ ë¶„ë¦¬í•œë‹¤.\n",
    "\n",
    "# ë‘˜ì§¸ ì„ë² ë”©ì€ êµ¬ê°„ ì„ë² ë”©ì´ë©°, ë¬¸ì¥ì„ êµ¬ë¶„í•œë‹¤.[CLS]ì™€ ì²« ë¬¸ì¥ì˜ í† í° ê·¸ë¦¬ê³  ì²« ë¬¸ì¥ì˜ ë [SEP]ê¹Œì§€ë¥¼ ë³´í†µ0, ë‚˜ë¨¸ì§€ë¥¼ 1ë¡œ ì„ë² ë”©\n",
    "# ì…‹ì§¸ ì„ë² ë”©ì€ ìœ„ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ì‹œí€€ìŠ¤ì—ì„œì˜ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì…€í”„ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ë¯€ë¡œ RNNê³¼ ë‹¬ë¦¬ ê° í† í°ì˜ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ëŠ” ì—†ë‹¤.\n",
    "# ë”°ë¼ì„œ ì´ë¥¼ ì„ë² ë”©ì— ëª…ì‹œì ìœ¼ë¡œ ë„£ì–´ì¤€ë‹¤. ë”°ë¼ì„œ ì¼ë°˜ì ìœ¼ë¡œ BERT í† í¬ë‚˜ì´ì €ê°€ ìœ„ì¹˜ ì„ë² ë”©ì„ ë°˜í™˜í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f35c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a beautiful day! í† í°í™” ê²°ê³¼: ['what', 'a', 'beautiful', 'day', '!']\n",
      "Nvidia Titan XP has 12GB of VRAM í† í°í™” ê²°ê³¼: ['n', '##vid', '##ia', 'titan', 'xp', 'has', '12', '##gb', 'of', 'vr', '##am']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence1 = 'What a beautiful day!'\n",
    "sentence2 = 'Nvidia Titan XP has 12GB of VRAM'\n",
    "\n",
    "# 1. í† í°í™” ê²°ê³¼\n",
    "print(sentence1, 'í† í°í™” ê²°ê³¼:', tokenizer.tokenize(sentence1))\n",
    "print(sentence2, 'í† í°í™” ê²°ê³¼:', tokenizer.tokenize(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1db273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT ì…ë ¥: {'input_ids': [[101, 2054, 1037, 3376, 2154, 999, 102, 0, 0, 0, 0, 0, 0], [101, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# input_ids í† í° ì„ë² ë”©ì˜ ê²°ê³¼\n",
    "# token_type_ids êµ¬ê°„ ì„ë² ë”©ì˜ ê²°ê³¼\n",
    "# attention_mask ì„ë² ë”©ì´ ì•„ë‹Œ ë§ˆìŠ¤í‚¹ê³¼ ê´€ë ¨ëœ ë¶€ë¶„ìœ¼ë¡œ, 0ì´ë©´ ë§ˆìŠ¤í‚¹ì´ ë˜ì–´ ì…€í”„ ì–´í…ì…˜ì—ì„œ ì œì™¸ë˜ê³ , 1ì´ë©´ ì…€í”„ ì–´í…ì…˜ í¬í•¨\n",
    "# ì¦‰ ì…ë ¥ í† í°ì—ì„œ ì…€í”„ ì–´í…ì…˜ì´ í•„ìš” ì—†ëŠ” ë¶€ë¶„ë“¤ì€ ë§ˆìŠ¤í‚¹ì„ í•œë‹¤.\n",
    "\n",
    "# 2. BERT ëª¨í˜• ì…ë ¥ ìƒì„±\n",
    "inputs = tokenizer([sentence1, sentence2], padding=True)\n",
    "print('BERT ì…ë ¥:', inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3c6707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‘ ë¬¸ì¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ BERT ì…ë ¥: {'input_ids': [101, 2054, 1037, 3376, 2154, 999, 102, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 101í† í°ì€ [CLS] , 102í† í°ì€ [SEP]\n",
    "\n",
    "# 3. ë‘ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì‹œí€€ìŠ¤ì— ëŒ€í•œ BERT ëª¨í˜• ì…ë ¥ ìƒì„±\n",
    "inputs = tokenizer(sentence1, sentence2, padding=True)\n",
    "print('ë‘ ë¬¸ì¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ BERT ì…ë ¥:', inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a99b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Leedonghoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count: 1600\n",
      "Test set count: 400\n"
     ]
    }
   ],
   "source": [
    "# íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íŠ¸ë ˆì´ë„ˆë¥¼ ì´ìš©í•œ ë¯¸ì„¸ì¡°ì •í•™ìŠµ\n",
    "# ë§¤ìš° ë³µì¡í•œ ê³¼ì •ì„ ê±°ì³ì•¼ í•˜ì§€ë§Œ ì„¸ë°€í•˜ê²Œ ëª¨í˜•ì„ ì¡°ì •í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ê³  ìì‹ ì´ ì›í•˜ëŠ” ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì¶”ê°€í•´ ëª¨í˜•ì„ í™•ì¥í•˜ëŠ” ê°€ëŠ¥\n",
    "# NLTK ì˜í™”ë¦¬ë·° ë°ì´í„° ë¯¸ì„¸ì¡°ì •í•™ìŠµ\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# sklearnì—ì„œ ì œê³µí•˜ëŠ” split í•¨ìˆ˜ë¥¼ ì‚¬ìš©\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "fileids = movie_reviews.fileids() # movie_reviews dataì—ì„œ file idë¥¼ ê°€ì ¸ì˜´\n",
    "\n",
    "# file idë¥¼ ì´ìš©í•´ raw text fileì„ ê°€ì ¸ì˜´\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "# labelì„ 0,1ì˜ ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "label_dict = {'pos':1, 'neg':0}\n",
    "y = [label_dict[c] for c in categories]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7)\n",
    "print('Train set count:',len(X_train))\n",
    "print('Test set count:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1774b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors = 'pt')\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc635dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key,val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "test_dataset = OurDataset(test_input, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7378dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leedonghoon\\AppData\\Local\\Temp\\ipykernel_1372\\3497906201.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµì„ ìˆ˜í–‰í•˜ê¸° ì „ì— ì •í™•ë„ ì¸¡ì •ì„ ìœ„í•œ ì¤€ë¹„ë¥¼ í•œë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì œê³µí•˜ëŠ” Trainer í´ë˜ìŠ¤ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµ ë„ì¤‘ì— ì†ì‹¤ì— ëŒ€í•œ ê°’ë§Œ ì œê³µ\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions = predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ce2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leedonghoon\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Leedonghoon\\AppData\\Local\\Temp\\ipykernel_1372\\994940297.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key,val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 35/400 6:54:47 < 76:27:49, 0.00 it/s, Epoch 0.17/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainerì™€ TrainingArguments ì •ì˜\n",
    "# TrainingArgumentsëŠ” í•™ìŠµì— ì‚¬ìš©í•  ë‹¤ì–‘í•œ ì˜µì…˜ê³¼ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•  ìˆ˜ ìˆê²Œ ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤ì´ë‹¤.\n",
    "# ë°˜ë“œì‹œ í•„ìš”í•œ ë„¤ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë§Œ ì •ì˜í•œë‹¤.\n",
    "# ì²«ì§¸, ì²´í¬í¬ì¸íŠ¸ ëª¨í˜•ì„ ì €ì¥í•  í´ë” ìœ„ì¹˜, ì²´í¬í¬ì¸íŠ¸ ëª¨í˜•ì€ ëª¨í˜•ì´ ê°€ì§„ ë³€ìˆ˜, ì¦‰ í•™ìŠµëœ ê²°ê³¼\n",
    "# ë‘˜ì§¸, í•™ìŠµ ì—í¬í¬ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í•™ìŠµ íšŸìˆ˜\n",
    "# ì…‹ì§¸, per_device_train_batch_sizeëŠ” í•™ìŠµì—ì„œ ì‚¬ìš©í•  ë°ì´í„° ë°°ì¹˜ì˜ í¬ê¸°\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Trainerì—ì„œ ì‚¬ìš©í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì§€ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/results', # ëª¨í˜• ì˜ˆì¸¡ì´ë‚˜ ì²´í¬í¬ì¸íŠ¸ ì¶œë ¥ í´ë”, ë°˜ë“œì‹œ í•„ìš”í•¨\n",
    "    num_train_epochs = 2,   # í•™ìŠµ ì—í¬í¬ ìˆ˜\n",
    "    per_device_train_batch_size = 8, # í•™ìŠµì— ì‚¬ìš©í•  ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    per_device_eval_batch_size = 16, # í‰ê°€ì— ì‚¬ìš©í•  ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,       # í•™ìŠµí•  ëª¨í˜•\n",
    "    args = training_args,     # ìœ„ì—ì„œ ì •ì˜í•œ í•™ìŠµ ë§¤ê°œë³€ìˆ˜\n",
    "    train_dataset = train_dataset,   # í•™ìŠµ ë°ì´í„°ì…‹\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "# ë¯¸ì„¸ì¡°ì •í•™ìŠµ ì‹¤í–‰\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89%ë¡œ ì§€ê¸ˆê¹Œì§€ë³´ë‹¤ í™•ì‹¤íˆ ê°œì„ ëœ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ì´ ê²°ê³¼ëŠ” í•™ìŠµí•  ë•Œ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ë˜ê±°ë‚˜ ë³€ê²½ë˜ëŠ” ë³€ìˆ˜ë“¤ì´ ìˆì–´ í•­ìƒ ê°™ì§€ëŠ” ì•Šë‹¤.\n",
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9675691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´í† ì¹˜ë¥¼ ì´ìš©í•œ ë¯¸ì„¸ì¡°ì •í•™ìŠµ\n",
    "# íŠ¸ëœìŠ¤í¬ë¨¸ì˜ Trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  íŒŒì´í† ì¹˜ë¡œ ë¯¸ì„¸ì¡°ì •í•™ìŠµì„ í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³¸ë‹¤.\n",
    "# TrainerëŠ” ë¹„êµì  ê°„í¸í•˜ê²Œ ì‚¬ì „í•™ìŠµ ëª¨í˜•ì— ëŒ€í•œ ë¯¸ì„¸ì¡°ì •í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆëŠ” ë°˜ë©´,\n",
    "# ëª¨í˜•ì„ í•„ìš”ì— ë”°ë¼ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤ëŠ” ì œí•œì´ ìˆë‹¤.\n",
    "# ë§Œì¼ BERTì˜ ê¸°ë³¸ ëª¨í˜•ì„ ì‚¬ìš©í•˜ê³  BERTê°€ ì œê³µí•˜ëŠ” [CLS] í† í°ì˜ ì¶œë ¥ê°’ì´ë‚˜, ëª¨ë“  ë‹¨ì–´ë“¤ì— ëŒ€í•œ ì¶œë ¥ê°’ì„ ì§ì ‘ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ íŒŒì´í† ì¹˜ë¥¼\n",
    "# ì´ìš©í•´ì„œ ì§ì ‘ ëª¨í˜•ì„ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df43cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77579e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ë¥˜ê¸°ê°€ ì—†ëŠ” ì›í˜•ì˜ BERT ëª¨í˜•ì— ì§ì ‘ ë¶„ë¥˜ê¸°ë¥¼ ì¶”ê°€í•´ ê°ì„± ë¶„ì„ì„ í•  ìˆ˜ ìˆëŠ” ì‚¬ìš©ìì •ì˜ ëª¨í˜•ì„ ë§Œë“¤ê³  ê·¸ ëª¨í˜•ì„ í•™ìŠµ\n",
    "# ì–¸ì–´ ëª¨ë¸ ì‚¬ì „í•™ìŠµ ëª¨í˜•ì¸ BertModel ì‚¬ìš©\n",
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT ì‚¬ì „í•™ìŠµ ëª¨í˜•ì„ í¬í•¨í•˜ëŠ” ì‹ ê²½ë§ ëª¨í˜•ì„ ì„ ì–¸\n",
    "# nun_labels ë¶„ë¥˜í•  í´ë˜ìŠ¤ì˜ ìˆ˜, ê°ì„±ë¶„ì„ 2\n",
    "# token_size BERT ëª¨í˜•ì˜ ì¶œë ¥ ë²¡í„°ì˜ í¬ê¸°, ë¬¸ì„œ ë¶„ë¥˜ì—ì„œëŠ” ëª¨ë“  ë‹¨ì–´ë“¤ì˜ ì¶œëµ(ì„ë² ë”©) ë²¡í„°ê°€ ì•„ë‹ˆë¼ CLS í† í°ì— ëŒ€ì‘í•˜ëŠ” ì¶œë ¥ ë²¡í„°ë§Œ\n",
    "# ê°€ì¥ ì•ì— ìˆëŠ” CSLí† í°ë§Œ outputs.last_hidden_state[:,0,:]\n",
    "# ë§Œì•½ ì–‘ë°©í–¥ LSTMì´ë‚˜ CNN ëª¨í˜•ì„ ë„£ê³  ì‹¶ë‹¤ë©´ ì¶”ê°€í•˜ë©´ ëœë‹¤.\n",
    "\n",
    "# BERTë¥¼ í¬í•¨í•œ ì‹ ê²½ë§ ëª¨í˜•\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "        # ë¶„ë¥˜ê¸° ì •ì˜\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # BERT ëª¨í˜•ì— ì…ë ¥ì„ ë„£ê³  ì¶œë ¥ì„ ë°›ìŒ\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT ì¶œë ¥ì—ì„œ CLS í† í°ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        return self.classifier(bert_clf_token)\n",
    "    \n",
    "# token_sizeëŠ” BERT í† í°ê³¼ ë™ì¼, bert_model.config.hidden_sizeë¡œ ì•Œ ìˆ˜ ìˆìŒ\n",
    "model = MyModel(bert_model, num_labels = 2, token_size=bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ\n",
    "# í•™ìŠµê³¼ì •ì„ ì´í•´í•˜ë ¤ë©´ ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ ì›ë¦¬ ë° ë‹¨ê³„ì™€ íŒŒì´í† ì¹˜ì— ëŒ€í•´ ì•Œì•„ì•¼ í•œë‹¤.\n",
    "# ë¨¼ì € GPU ê°€ì†ì„ í™œì„±í™”í•˜ê³  ëª¨í˜•ì„ GPUë¡œ ë³µì‚¬í•œë‹¤. ë‹¤ìŒì€ í•™ìŠµì„ ìœ„í•´ ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.\n",
    "# ê° ì—í¬í¬ì—ì„œëŠ” ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ëª¨í˜•ìœ¼ë¡œ ë°°ì¹˜ì˜ ì…ë ¥ê°’ì— ëŒ€í•´ ì˜ˆì¸¡ì„ í•œ í›„, ë‹µ(labels)ê³¼ ë¹„êµí•´ ì†ì‹¤ì„ ê³„ì‚°í•œë‹¤.\n",
    "# ì´ì— ë”°ë¼ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³  ëª¨í˜•ì˜ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•œë‹¤.\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# GPU ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©´ deviceë¥¼ cudaë¡œ ì„¤ì •í•˜ê³ , ì•„ë‹ˆë©´ cpuë¡œ ì„¤ì •\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device) # ëª¨í˜•ì„ GPUë¡œ ë³µì‚¬\n",
    "model.train() # í•™ìŠµëª¨ë“œë¡œ ì „í™˜\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì œê³µí•˜ëŠ” AdamWë¡œ ì„¤ì •\n",
    "optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# ë©€í‹°í´ë˜ìŠ¤ì´ë¯€ë¡œ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time() # ì‹œì‘ì‹œê°„ ê¸°ë¡\n",
    "num_epochs = 4  # í•™ìŠµ epochë¥¼ 4íšŒë¡œ ì„¤ì •\n",
    "for epoch in range(num_epochs):\n",
    "    total_epoch_loss = 0 # epochì˜ ì´ loss ì´ˆê¸°í™”\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optim.zero_grad() # ê·¸ë ˆì´ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        \n",
    "        # ë°°ì¹˜ì—ì„œ ë¼ë²¨ì„ ì œì™¸í•œ ì…ë ¥í•œ ì¶”ì¶œí•´ GPUë¡œ ë³µì‚¬\n",
    "        inputs= {k:v.to(device) for k,v in batch.items() if k!='labels'}\n",
    "        \n",
    "        labels = batch['labels'].to(device) # ë°°ì¹˜ì—ì„œ ë¼ë²¨ì„ ì¶”ì¶œí•´ GPUë¡œ ë³µì‚¬\n",
    "        outputs = model(inputs) # ëª¨í˜•ìœ¼ë¡œ ê²°ê³¼ ì˜ˆì¸¡\n",
    "        \n",
    "        # ë‘ í´ë˜ìŠ¤ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê³  ê°ê° ë¹„êµí•´ì•¼ í•˜ë¯€ë¡œ\n",
    "        # labelsì— ëŒ€í•´ ì›í•« ì¸ì½”ë”©ì„ ì ìš©í•œ í›„ì— ì†ì‹¤ì„ ê³„ì‚°\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss ê³„ì‚°\n",
    "        \n",
    "        if (step + 1) % 100 ==0 : # 100 ë°°ì¹˜ë§ˆë‹¤ ê²½ê³¼í•œ ì‹œê°„ê³¼ lossë¥¼ ì¶œë ¥\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                'Epoch %d, batch %d, elapsed time: % .2f, loss: %.4f'\n",
    "                        % (epoch +1, step +1, elapsed, loss)\n",
    "            )\n",
    "            total_epoch_loss += loss\n",
    "            loss.backward() # ê·¸ë ˆì´ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "            optim.step() # ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "            \n",
    "        avg_epoch_loss = total_epoch_loss / len(train_loader) # epochì˜ í‰ê·  loss ê³„ì‚°\n",
    "        print('Average loss for epoch %d: %.4f' % (epoch+1, avg_epoch_loss))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb56570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerë¥¼ ì´ìš©í•œ ë¯¸ì„¸ì¡°ì •í•™ìŠµì˜ ì„±ëŠ¥ì¸ 87%ë³´ë‹¤ë„ ë‚˜ì€ ì„±ëŠ¥ì´ë‹¤. ì˜µí‹°ë§ˆì´ì €ì˜ í•™ìŠµì „ëµì„ ìˆ˜ì •í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥ì˜ í–¥ìƒì´ ê°€ëŠ¥í•˜ë¯€ë¡œ\n",
    "# íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì œê³µí•˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì ìš©í•œë‹¤ë©´ ì¢€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤.\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§‘í•©ì— ëŒ€í•´ ì„±ëŠ¥ ì¸¡ì •\n",
    "from datasets import load_metric\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "metric = load_metric('accuracy')\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k,v in batch.items() if k !='labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad(): # í•™ìŠµí•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ê·¸ë ˆì´ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë”\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    metric.add_batch(predictions, references=labels)\n",
    "    \n",
    "metric.compute()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b440ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a516c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b556d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e065e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce9a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
