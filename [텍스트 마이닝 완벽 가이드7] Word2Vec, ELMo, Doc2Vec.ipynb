{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f333e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 워드 임베딩과 관련한 대표적인 기법인 Word2Vec, ELMo, 워드가 아닌 직접 임베딩하는 Doc2Vec\n",
    "\n",
    "# Word2Vec 기법에서 학습의 목적은 단어에 의미적인 정보를 함축함으로써 유사도를 계산하거나 연산을 수행하고, 더 나아가서 학습된 결과를 다른\n",
    "# 작업에서도 사용할 수 있는 전이학습을 지원하는 것이다.\n",
    "# 다음 단어 혹은 주변 단어에 대한 예측을 잘할 수 있도록 학습함으로써 문맥을 이해시키고, 밀집 벡터에 그러한 문맥 정보를 담으려는 시도\n",
    "# CBOW, Skip-Gram의 두가지 학습 방식을 갖고 있다.\n",
    "# CBOW : 주변의 단어를 이용해 중심단어를 예측하도록 학습을 수행\n",
    "# 앞뒤 단어들을 몇 개씩 예측에 사용할지 결정하는 범위를 윈도우(window)라고 한다. \n",
    "# Skip-Gram : 중심의 한 단어를 이용해 주변의 단어를 예측한다.\n",
    "\n",
    "# Word2Vec 활용\n",
    "# 미리 학습된 gensim data를 다운로드\n",
    "import gensim.downloader as api\n",
    "wv = api.load('glove-wiki-gigaword-50')\n",
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a832942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Size of the vector: 50\n",
      "#Vector for king: [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n"
     ]
    }
   ],
   "source": [
    "vec_king = wv['king']\n",
    "print('#Size of the vector:', len(vec_king))\n",
    "print('#Vector for king:', vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125c9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53093773 vs 0.41133785\n",
      "0.22956748 vs 0.60031056\n",
      "미니밴에 가까운 차: [('truck', 0.9100273251533508), ('suv', 0.9040074944496155), ('jeep', 0.8619830012321472)]\n",
      "여성, 왕에는 가까우면서 남성과는 먼 단어: [('queen', 0.8523604273796082)]\n",
      "breakfast cereal dinner lunch 중에서 다른 단어들과의 거리가 가장 먼 단어: cereal\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('king','man'), 'vs', wv.similarity('king', 'woman'))\n",
    "print(wv.similarity('queen', 'wan'), 'vs', wv.similarity('queen', 'woman'))\n",
    "print('미니밴에 가까운 차:', wv.most_similar(positive=['car', 'minivan'], topn=3))\n",
    "print('여성, 왕에는 가까우면서 남성과는 먼 단어:', wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "print('breakfast cereal dinner lunch 중에서 다른 단어들과의 거리가 가장 먼 단어:',\n",
    "     wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2350d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between cat and dog:0.08\n",
      "0.5375\n",
      "0.5627\n",
      "0.4377\n"
     ]
    }
   ],
   "source": [
    "# distance 유사도의 반대 개념인 거리를 반환, n_similarity는 단어집합 간의 유사도 계산\n",
    "print(\"distance between cat and dog:{:.2f}\".format(wv.distance('cat','dog')))\n",
    "print(\"{:.4f}\".format(wv.n_similarity(['bulgogi', 'shop'], ['japanese', 'restaurant'])))\n",
    "print(\"{:.4f}\".format(wv.n_similarity(['bulgogi', 'shop'], ['korean', 'restaurant'])))\n",
    "print(\"{:.4f}\".format(wv.n_similarity(['bulgogi', 'shop'], ['french', 'restaurant'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec의 가장 큰 문제점은 동음이의어이다.\n",
    "# Word2Vec에서는 임베딩 벡터가 고정돼 있지만 ELMo에서는 가변적이라는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc2c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0265cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0108b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140cb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ce3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648361a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
